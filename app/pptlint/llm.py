"""
LLM æ¨¡å—ï¼ˆé»˜è®¤å¯ç”¨ï¼Œå¯é™çº§ï¼‰ã€‚

è¯´æ˜ï¼š
- æ¥å£é‡‡ç”¨ç®€åŒ–çš„é€‚é…å™¨é£æ ¼ï¼Œä¼˜å…ˆèµ° OpenAI-compatible endpointï¼ˆå¦‚ DeepSeekï¼‰ã€‚
- è‹¥æœªé…ç½® API KEY æˆ–è¯·æ±‚å¤±è´¥ï¼Œè¿”å›ç©ºå»ºè®®ï¼Œè°ƒç”¨æ–¹éœ€è‡ªåŠ¨é™çº§ä¸ºè§„åˆ™å»ºè®®ã€‚
"""
import os
import json
from typing import Any, Dict, List, Optional
import urllib.request


def _resolve_endpoint(provider: str, model: Optional[str], explicit_endpoint: Optional[str]) -> Optional[str]:
    """æ ¹æ®æä¾›å•†å’Œæ¨¡å‹åæ¨æ–­é»˜è®¤ endpointï¼ˆOpenAI-compatibleï¼‰ï¼Œæ˜¾å¼å€¼ä¼˜å…ˆã€‚"""
    if explicit_endpoint:
        return explicit_endpoint
    
    provider_lower = provider.lower()
    model_lower = (model or "").lower()
    
    # å¸¸è§æä¾›æ–¹çš„é»˜è®¤ OpenAI-compatible chat.completions ç«¯ç‚¹
    if provider_lower == "deepseek" or "deepseek" in model_lower:
        return os.getenv("LLM_ENDPOINT", "https://api.deepseek.com/v1/chat/completions")
    elif provider_lower == "openai" or model_lower.startswith("gpt"):
        return os.getenv("LLM_ENDPOINT", "https://api.openai.com/v1/chat/completions")
    elif provider_lower == "anthropic" or "claude" in model_lower:
        return os.getenv("LLM_ENDPOINT", "https://api.anthropic.com/v1/messages")
    elif provider_lower == "local":
        return os.getenv("LLM_ENDPOINT", "http://localhost:11434/v1/chat/completions")  # Ollamaé»˜è®¤ç«¯ç‚¹
    else:
        # å…œåº•ç”¨ç¯å¢ƒå˜é‡
        return os.getenv("LLM_ENDPOINT")


class LLMClient:
    def __init__(self, provider: str = "deepseek", endpoint: Optional[str] = None, 
                 api_key: Optional[str] = None, model: Optional[str] = None,
                 temperature: float = 0.2, max_tokens: int = 1024,
                 use_proxy: bool = False, proxy_url: Optional[str] = None):
        self.provider = provider
        self.model = model or "deepseek-chat"
        self.endpoint = _resolve_endpoint(self.provider, self.model, endpoint)
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # ä»£ç†é…ç½®
        self.use_proxy = use_proxy
        self.proxy_url = proxy_url
        
        # æ ¹æ®æä¾›å•†è®¾ç½®API key
        if api_key:
            self.api_key = api_key
        else:
            # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–å¯¹åº”æä¾›å•†çš„API key
            env_key = f"{provider.upper()}_API_KEY"
            self.api_key = os.getenv(env_key, "")

    def complete(self, prompt: str, max_tokens: Optional[int] = None, stop_event: Optional[object] = None) -> str:
        try:
            if not self.api_key:
                print("æœªé…ç½®APIå¯†é’¥ï¼ŒLLMåŠŸèƒ½å°†ä¸å¯ç”¨")
                return ""
            
            req = urllib.request.Request(self.endpoint, method="POST")
            req.add_header("Content-Type", "application/json")
            req.add_header("Authorization", f"Bearer {self.api_key}")
            
            # æ¯æ¬¡è°ƒç”¨éƒ½ä½¿ç”¨æ–°çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œé¿å…å†å²å¯¹è¯å¹²æ‰°
            body = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant for document review."},
                    {"role": "user", "content": prompt},
                ],
                "max_tokens": max_tokens or self.max_tokens,
                "temperature": self.temperature,
            }
            
            data = json.dumps(body).encode("utf-8")
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
            if stop_event and stop_event.is_set():
                print("â¹ï¸ LLMè°ƒç”¨è¢«ç”¨æˆ·ç»ˆæ­¢")
                return ""
            
            try:
                # é…ç½®ä»£ç†å¤„ç†
                if self.use_proxy and self.proxy_url:
                    # å¯ç”¨ä»£ç†
                    proxy_handler = urllib.request.ProxyHandler({
                        'http': self.proxy_url,
                        'https': self.proxy_url
                    })
                    opener = urllib.request.build_opener(proxy_handler)
                    urllib.request.install_opener(opener)
                    print(f"ğŸŒ ä½¿ç”¨ä»£ç†: {self.proxy_url}")
                else:
                    # ç¦ç”¨ä»£ç†ï¼Œæ¸…é™¤ç¯å¢ƒå˜é‡å½±å“
                    proxy_handler = urllib.request.ProxyHandler({})
                    opener = urllib.request.build_opener(proxy_handler)
                    urllib.request.install_opener(opener)
                
                with urllib.request.urlopen(req, data=data) as resp:
                    payload = json.loads(resp.read().decode("utf-8"))
                    # OpenAI style
                    return payload.get("choices", [{}])[0].get("message", {}).get("content", "")
            except Exception as e:
                print(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
                return ""
        except Exception as e:
            print(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
            return ""


def suggest_japanese_fluency(llm: LLMClient, text: str, constraints: str = "") -> List[str]:
    prompt = f"æ”¹å†™ä¸ºè‡ªç„¶æµç•…çš„æ—¥æœ¬æ±½è½¦ITè¡Œä¸šè¡¨è¿°ï¼Œä¿æŒæŠ€æœ¯å‡†ç¡®æ€§ï¼š\nçº¦æŸ:{constraints}\næ–‡æœ¬:\n{text}"
    out = llm.complete(prompt)
    return [s.strip() for s in out.splitlines() if s.strip()] if out else []


def suggest_logic_transition(llm: LLMClient, outline: str) -> List[str]:
    prompt = f"ä¸ºä»¥ä¸‹PPTå¤§çº²æå‡ºè¿‡æ¸¡ä¸è¿è´¯æ€§å»ºè®®ï¼ˆç®€çŸ­è¦ç‚¹ï¼‰ï¼š\n{outline}"
    out = llm.complete(prompt)
    return [s.strip() for s in out.splitlines() if s.strip()] if out else []


def suggest_term_unification(llm: LLMClient, variants: List[str]) -> Optional[str]:
    prompt = "è¯·åœ¨ä»¥ä¸‹æœ¯è¯­å˜ä½“ä¸­é€‰æ‹©ç»Ÿä¸€ç”¨æ³•ï¼ˆåªè¾“å‡ºä¸€ä¸ªæœ€ä½³å†™æ³•ï¼‰ï¼š\n" + "\n".join(variants)
    out = llm.complete(prompt)
    return out.strip() if out else None


