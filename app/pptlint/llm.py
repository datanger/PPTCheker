"""
LLM æ¨¡å—ï¼ˆé»˜è®¤å¯ç”¨ï¼Œå¯é™çº§ï¼‰ã€‚

è¯´æ˜ï¼š
- æ¥å£é‡‡ç”¨ç®€åŒ–çš„é€‚é…å™¨é£æ ¼ï¼Œä¼˜å…ˆèµ° OpenAI-compatible endpointï¼ˆå¦‚ DeepSeekï¼‰ã€‚
- è‹¥æœªé…ç½® API KEY æˆ–è¯·æ±‚å¤±è´¥ï¼Œè¿”å›ç©ºå»ºè®®ï¼Œè°ƒç”¨æ–¹éœ€è‡ªåŠ¨é™çº§ä¸ºè§„åˆ™å»ºè®®ã€‚
"""
import os
import json
import ssl
from typing import Any, Dict, List, Optional
import urllib.request


def _resolve_base_url(provider: str, model: Optional[str], explicit_base_url: Optional[str]) -> Optional[str]:
    """æ ¹æ®æä¾›å•†ä¸æ¨¡å‹æ¨æ–­é»˜è®¤ base urlï¼ˆæ˜¾å¼å€¼ä¼˜å…ˆï¼‰ã€‚"""
    if explicit_base_url:
        return explicit_base_url

    provider_lower = (provider or "").lower()
    model_lower = (model or "").lower()

    # å¸¸è§æä¾›å•†é»˜è®¤ base urlï¼ˆProvider ä¼˜å…ˆï¼‰
    if provider_lower == "local":
        # å†…ç½‘LLMæœåŠ¡é»˜è®¤åœ°å€
        return os.getenv("LLM_BASE_URL", "https://192.168.10.173/sdw/chatbot/sysai/v1")
    if provider_lower == "ollama":
        # Ollama æœ¬åœ°æœåŠ¡
        return os.getenv("LLM_BASE_URL", "http://localhost:11434/v1")
    if provider_lower == "deepseek":
        return os.getenv("LLM_BASE_URL", "https://api.deepseek.com/v1")
    if provider_lower == "openai":
        return os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
    if provider_lower == "anthropic":
        # Anthropic å¹¶éä¸¥æ ¼ OpenAI å…¼å®¹ï¼Œä½†æ­¤å¤„ä»è¿”å›å…¶ messages æ ¹è·¯å¾„
        return os.getenv("LLM_BASE_URL", "https://api.anthropic.com/v1")
    if provider_lower in ("kimi", "moonshot"):
        # Kimi (Moonshot) é‡‡ç”¨ OpenAI å…¼å®¹æ¥å£
        return os.getenv("LLM_BASE_URL", "https://api.moonshot.cn/v1")
    if provider_lower in ("bailian", "dashscope", "aliyun"):
        # é˜¿é‡Œäº‘ç™¾ç‚¼ DashScope å…¼å®¹æ¨¡å¼
        return os.getenv("LLM_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
    
    # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ providerï¼Œåˆ™æ ¹æ®æ¨¡å‹åç§°æ¨æ–­
    if "deepseek" in model_lower:
        return os.getenv("LLM_BASE_URL", "https://api.deepseek.com/v1")
    if model_lower.startswith("gpt"):
        return os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
    if "claude" in model_lower:
        return os.getenv("LLM_BASE_URL", "https://api.anthropic.com/v1")
    if "moonshot" in model_lower:
        return os.getenv("LLM_BASE_URL", "https://api.moonshot.cn/v1")
    if "qwen" in model_lower:
        return os.getenv("LLM_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
    return os.getenv("LLM_BASE_URL")


def _resolve_endpoint(provider: str, model: Optional[str], explicit_endpoint: Optional[str], base_url: Optional[str]) -> Optional[str]:
    """æ¨æ–­æœ€ç»ˆ endpointï¼›è‹¥ç»™å‡ºæ˜¾å¼ endpoint åˆ™ä½¿ç”¨ä¹‹ï¼Œå¦åˆ™ç”± base_url + è·¯å¾„ç»„åˆã€‚"""
    if explicit_endpoint:
        return explicit_endpoint

    base = base_url or _resolve_base_url(provider, model, None)
    provider_lower = (provider or "").lower()
    model_lower = (model or "").lower()

    if not base:
        return None

    # OpenAI å…¼å®¹è·¯å¾„
    if provider_lower in ("deepseek", "openai", "kimi", "moonshot", "bailian", "dashscope", "aliyun", "local") or any(
        k in model_lower for k in ("gpt", "deepseek", "qwen", "moonshot", "llama", "qwen2")
    ):
        return f"{base.rstrip('/')}/chat/completions"

    # Anthropic messages
    if provider_lower == "anthropic" or "claude" in model_lower:
        return f"{base.rstrip('/')}/messages"

    # å…œåº•ï¼šå‡å®š OpenAI å…¼å®¹
    return f"{base.rstrip('/')}/chat/completions"


class LLMClient:
    def __init__(self, provider: str = "deepseek", endpoint: Optional[str] = None, 
                 api_key: Optional[str] = None, model: Optional[str] = None,
                 temperature: float = 0.2, max_tokens: int = 1024,
                 use_proxy: bool = False, proxy_url: Optional[str] = None,
                 base_url: Optional[str] = None):
        self.provider = provider
        self.model = model or "deepseek-chat"
        self.base_url = _resolve_base_url(self.provider, self.model, base_url)
        self.endpoint = _resolve_endpoint(self.provider, self.model, endpoint, self.base_url)
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # ä»£ç†é…ç½®
        self.use_proxy = use_proxy
        self.proxy_url = proxy_url
        
        # æ ¹æ®æä¾›å•†è®¾ç½®API key
        if api_key:
            self.api_key = api_key
        else:
            # ä¸ºlocalå’Œollamaè®¾ç½®é»˜è®¤API key
            provider_lower = provider.lower()
            if provider_lower == "local":
                self.api_key = "local-api-key"
            elif provider_lower == "ollama":
                self.api_key = "ollama-api-key"
            else:
                # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–å¯¹åº”æä¾›å•†çš„API key
                env_key = f"{provider.upper()}_API_KEY"
                self.api_key = os.getenv(env_key, "")

    def complete(self, prompt: str, max_tokens: Optional[int] = None, stop_event: Optional[object] = None) -> str:
        try:
            if not self.api_key:
                print("æœªé…ç½®APIå¯†é’¥ï¼ŒLLMåŠŸèƒ½å°†ä¸å¯ç”¨")
                return ""
            
            req = urllib.request.Request(self.endpoint, method="POST")
            req.add_header("Content-Type", "application/json")
            req.add_header("Authorization", f"Bearer {self.api_key}")
            
            # æ¯æ¬¡è°ƒç”¨éƒ½ä½¿ç”¨æ–°çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œé¿å…å†å²å¯¹è¯å¹²æ‰°
            body = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant for document review."},
                    {"role": "user", "content": prompt},
                ],
                "max_tokens": max_tokens or self.max_tokens,
                "temperature": self.temperature,
            }
            
            data = json.dumps(body).encode("utf-8")
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
            if stop_event and stop_event.is_set():
                print("â¹ï¸ LLMè°ƒç”¨è¢«ç”¨æˆ·ç»ˆæ­¢")
                return ""
            
            try:
                # é…ç½®ä»£ç†å¤„ç†
                if self.use_proxy and self.proxy_url:
                    # å¯ç”¨ä»£ç†
                    proxy_handler = urllib.request.ProxyHandler({
                        'http': self.proxy_url,
                        'https': self.proxy_url
                    })
                    opener = urllib.request.build_opener(proxy_handler)
                    urllib.request.install_opener(opener)
                    print(f"ğŸŒ ä½¿ç”¨ä»£ç†: {self.proxy_url}")
                else:
                    # ç¦ç”¨ä»£ç†ï¼Œæ¸…é™¤ç¯å¢ƒå˜é‡å½±å“
                    proxy_handler = urllib.request.ProxyHandler({})
                    opener = urllib.request.build_opener(proxy_handler)
                    urllib.request.install_opener(opener)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡SSLéªŒè¯ï¼ˆå†…ç½‘åœ°å€ï¼‰
                context = None
                if self.endpoint and ("192.168." in self.endpoint or "10." in self.endpoint or "172." in self.endpoint):
                    context = ssl.create_default_context()
                    context.check_hostname = False
                    context.verify_mode = ssl.CERT_NONE
                    print(f"ğŸ”“ è·³è¿‡SSLéªŒè¯: {self.endpoint}")
                
                with urllib.request.urlopen(req, data=data, context=context) as resp:
                    payload = json.loads(resp.read().decode("utf-8"))
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                    if "error" in payload and payload["error"]:
                        print(f"LLM APIé”™è¯¯: {payload['error'].get('message', 'æœªçŸ¥é”™è¯¯')}")
                        return ""
                    
                    # OpenAI style - å®‰å…¨è§£æ
                    choices = payload.get("choices", [])
                    if choices and len(choices) > 0:
                        message = choices[0].get("message", {})
                        return message.get("content", "")
                    
                    print("LLMæœªè¿”å›æœ‰æ•ˆå†…å®¹")
                    return ""
            except Exception as e:
                print(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
                return ""
        except Exception as e:
            print(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
            return ""


def suggest_japanese_fluency(llm: LLMClient, text: str, constraints: str = "") -> List[str]:
    prompt = f"æ”¹å†™ä¸ºè‡ªç„¶æµç•…çš„æ—¥æœ¬æ±½è½¦ITè¡Œä¸šè¡¨è¿°ï¼Œä¿æŒæŠ€æœ¯å‡†ç¡®æ€§ï¼š\nçº¦æŸ:{constraints}\næ–‡æœ¬:\n{text}"
    out = llm.complete(prompt)
    return [s.strip() for s in out.splitlines() if s.strip()] if out else []


def suggest_logic_transition(llm: LLMClient, outline: str) -> List[str]:
    prompt = f"ä¸ºä»¥ä¸‹PPTå¤§çº²æå‡ºè¿‡æ¸¡ä¸è¿è´¯æ€§å»ºè®®ï¼ˆç®€çŸ­è¦ç‚¹ï¼‰ï¼š\n{outline}"
    out = llm.complete(prompt)
    return [s.strip() for s in out.splitlines() if s.strip()] if out else []


def suggest_term_unification(llm: LLMClient, variants: List[str]) -> Optional[str]:
    prompt = "è¯·åœ¨ä»¥ä¸‹æœ¯è¯­å˜ä½“ä¸­é€‰æ‹©ç»Ÿä¸€ç”¨æ³•ï¼ˆåªè¾“å‡ºä¸€ä¸ªæœ€ä½³å†™æ³•ï¼‰ï¼š\n" + "\n".join(variants)
    out = llm.complete(prompt)
    return out.strip() if out else None


